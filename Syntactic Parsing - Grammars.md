![[08-syntactic-grammars.pdf]]

## Notes
* **Syntactic Parsing**
	* *Goal and motivation*
		* *Syntax*: the combination of words in sentences
		* *Syntactic parsing* : extract information of the combination of words (*the syntactic structure*).
		* *Applications:*
			* Authorship recognition
			* Grammer checking
			* Machine translation
			* Information extraction
		* *Goal*: find the syntactic structure associated to a sentence
	* *Types of syntactic structures:*
		* Depending on the language on the task we can need one or the other.
		* *Constituent tree*
			* ![[Pasted image 20231109141615.png]]
			* The constituencies are sub-trees of the tree (i.e. subjects, verb+subject, etc)
			* Constituents = abstract linguistic units
			* ![[Pasted image 20231109142039.png]]
		* *Dependency tree*
			* From *verbs* all other relations emerge
			* Focus in *relations between words*
			* Handles *free word order* nicely
			* This is more flexible that with the constituent tree
				* ![[Pasted image 20231109142115.png]]
* **Context Free Grammars (CFG)**
	* *Definition*
		* A context free grammar $G=(N\Sigma,R,S)$ where:
			* $N$ is a set of *non-terminal symbols*
			* $\Sigma$ is a set of *terminal symbols*
			* $R$ is a set of *rules* to the form $X\rightarrow Y_1Y_2...Y_n$ for $n\geq 0, X\in N, Y_{i}\in (N\cup \Sigma)$ 
			* $S\in N$ is a distinguished start symbol
	* *Example of  a CFG*
		* With this: $S=sentence$, $VP=verb phrase$, $NP=noun phrase$, $PP=prepositional\; phrase,$ $DT=determiner$, $Vi=intransitive\; verb$, $Vt=transitive\; verb$, $NN=noun$, $IN=preposition$
		* $N=\{S,NP,VP,PP,DT,Vi,Vt,NN,IN\}$
		* $S=S$
		* $\Sigma=\{sleeps,saw,man,woman,telescope,the,with,in\}$
		* $R = $
			* ![[Pasted image 20231109145533.png]]
			* ![[Pasted image 20231109145543.png]]
	* *Left-most derivations in CFGs*
		* A left-most derivation is a sequence of strings $s_1...s_n$, where 
			* $s_1=S$, the start symbol 
			* $s_n \in \Sigma^*$, i.e. $s_n$ is made up of terminal symbols only 
		* Each $s_i$ for $i=2...n$ is derived from $s_{i-1}$ by picking the left-most non-terminal $X$ in $s_{i-1}$ and replacing it by some $\beta$ where $X\rightarrow\beta$ is a rule in $R$
		* *Example*
			* \[S\], \[NP VP\], \[D N VP\], \[the N VP\], \[the man VP\], \[the man Vi\], \[the man sleeps\]
			* Representation of a derivation as a tree
				* ![[Pasted image 20231109150540.png]]
	* *Properties of CFGs*
		* A CFG defines a set of possible derivations 
		* A string $s\in\Sigma^*$ is in the language defined by the CFG if there is at least one derivation which yields $s$
		* Each string in the language generated by the CFG may have more than one derivation (“ambiguity”)
	* *Ambiguities*
		* Look for the syntactic ambiguities for the next sentences, the last part of them can be a modifier of the object or of the verb
			* I cleaned the dishes *from dinner* 
			* I cleaned the dishes *with detergent* 
			* I cleaned the dishes *in my pajamas* 
			* I cleaned the dishes *in the sink*
	* *Excercise* $\rightarrow$ 1st in[[Exercises about CFGs]]
* **Probabilistic Context Free Grammars**
	* We compute the probability of a possible tree $t$ with rules $$\alpha_1\rightarrow\beta_1,\alpha_2\rightarrow\beta_2,...,\alpha_n\rightarrow\beta_n$$ is $$p(t)=\prod_{i=1}^{n}q(\alpha_i\rightarrow\beta_i)$$ where $q(\alpha\rightarrow\beta)$ is the probability for a rule $\alpha\rightarrow\beta$ 
	* *Definition*
		* A context free grammar $G=(N,\Sigma,S,R)$
		* A parameter $$q(\alpha\rightarrow\beta)$$ for each rule $\alpha\rightarrow\beta\in R$. The parameter q(α → β) $q(\alpha\rightarrow\beta)$ can be interpreted as the conditional probabilty of choosing rule $\alpha\rightarrow\beta$ in a left-most derivation, given that the non-terminal being expanded is $\alpha$. For any $X\in N$ , we have the constraint $$\sum\limits_{\alpha\rightarrow\in R:\alpha=X}q(\alpha\rightarrow\beta) = 1$$ In addition we have $q(\alpha\rightarrow\beta)\geq 0$ for any $\alpha\rightarrow\beta\in R$. 
		* Given a parse-tree $t \in \tau_G$ containing tules $\alpha_1\rightarrow\beta_{1,}\alpha_2\rightarrow\beta_2,...,\alpha_n\rightarrow\beta_n$ the probability of $t$ under the PCFG is $$p(t)=\prod_{i=1}^{n}q(\alpha_i\rightarrow\beta_i)$$ 
	* *Properties*
		* Assigns a probability to each *left-most derivation*, or parse-tree, allowed by the underlying CFG 
		* Say we have a sentence $s$, set of derivations for that sentence is $\tau(s)$. Then a PCFG assigns a probability $p(t)$ to each member of $\tau(s)$. i.e., we now have a *ranking in order of probability*. 
		* The *most likely parse tree* for a sentence $s$ is $arg max_{t\in\tau} p(t)$
	* *Learning Treebank Grammars*
		* Read the grammar rules from a treebank
			* ![[Pasted image 20231109153431.png]]
		* Set rule weights by maximum likelihood 
		* Other approaches are out of this course: PCFG with parent annotations, lexicalized PCFG, PCFG with latent variables
	* *Maximum Likelihood Estimates*
		* Algorithm 
			1. Given a treebank, define a CFG by *taking all rules seen in the treebank*.
			2. Maximum Likelihood estimates $$q(\alpha\rightarrow\beta)=\frac{Count(\alpha\rightarrow\beta)}{Count(\alpha)}$$ where the *counts are taken from the examples in the treebank*. 
		* Smoothing issues apply here 
		* Having the appropriate CFG is critical to success
	* *Exercise* $\rightarrow$ 2nd in [[Exercises about CFGs]]