---
annotation-target: "[[2106.06437.pdf]]"
---


>%%
>```annotation-json
>{"created":"2023-12-13T13:20:43.408Z","updated":"2023-12-13T13:20:43.408Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":501,"end":683},{"type":"TextQuoteSelector","exact":"There are many motivationsfor feature selection, it may result in better models, it may provide insight into thedata and it may deliver economies in data gathering or data processing","prefix":"t to use for model development. ","suffix":". For thesereasons feature selec"}]}]}
>```
>%%
>*%%PREFIX%%t to use for model development.%%HIGHLIGHT%% ==There are many motivationsfor feature selection, it may result in better models, it may provide insight into thedata and it may deliver economies in data gathering or data processing== %%POSTFIX%%. For thesereasons feature selec*
>%%LINK%%[[#^txfy5jofrn|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^txfy5jofrn


>%%
>```annotation-json
>{"created":"2023-12-13T13:21:08.220Z","updated":"2023-12-13T13:21:08.220Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":1684,"end":1702},{"type":"TextQuoteSelector","exact":" important because","prefix":"ese methods.Feature selection is","suffix":" it can deliver a number of bene"}]}]}
>```
>%%
>*%%PREFIX%%ese methods.Feature selection is%%HIGHLIGHT%% ==important because== %%POSTFIX%%it can deliver a number of bene*
>%%LINK%%[[#^zdzam5pvweq|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^zdzam5pvweq


>%%
>```annotation-json
>{"created":"2023-12-13T13:21:12.154Z","updated":"2023-12-13T13:21:12.154Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":1741,"end":1759},{"type":"TextQuoteSelector","exact":"Better classifiers","prefix":" deliver a number of benefits:• ","suffix":": The obvious benefit of feature"}]}]}
>```
>%%
>*%%PREFIX%%deliver a number of benefits:•%%HIGHLIGHT%% ==Better classifiers== %%POSTFIX%%: The obvious benefit of feature*
>%%LINK%%[[#^cy7mrk2m14v|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^cy7mrk2m14v


>%%
>```annotation-json
>{"created":"2023-12-13T13:21:18.713Z","updated":"2023-12-13T13:21:18.713Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":2032,"end":2051},{"type":"TextQuoteSelector","exact":"Knowledge discovery","prefix":"ned to berobust against noise.• ","suffix":": Perhaps the most enduring bene"}]}]}
>```
>%%
>*%%PREFIX%%ned to berobust against noise.•%%HIGHLIGHT%% ==Knowledge discovery== %%POSTFIX%%: Perhaps the most enduring bene*
>%%LINK%%[[#^wxfo8tqqdh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^wxfo8tqqdh


>%%
>```annotation-json
>{"created":"2023-12-13T13:21:25.038Z","updated":"2023-12-13T13:21:25.038Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":2234,"end":2249},{"type":"TextQuoteSelector","exact":"Data Gathering:","prefix":"eaches us a lotabout the data.• ","suffix":" In domains where data comes at "}]}]}
>```
>%%
>*%%PREFIX%%eaches us a lotabout the data.•%%HIGHLIGHT%% ==Data Gathering:== %%POSTFIX%%In domains where data comes at*
>%%LINK%%[[#^x61784ndxje|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^x61784ndxje


>%%
>```annotation-json
>{"created":"2023-12-13T13:21:29.707Z","updated":"2023-12-13T13:21:29.707Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":2411,"end":2429},{"type":"TextQuoteSelector","exact":"Computational Cost","prefix":"ification task can save money.• ","suffix":": Identifying minimal feature su"}]}]}
>```
>%%
>*%%PREFIX%%ification task can save money.•%%HIGHLIGHT%% ==Computational Cost== %%POSTFIX%%: Identifying minimal feature su*
>%%LINK%%[[#^0jxxr33wasv7|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^0jxxr33wasv7


>%%
>```annotation-json
>{"created":"2023-12-13T13:21:36.483Z","updated":"2023-12-13T13:21:36.483Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":2576,"end":2603},{"type":"TextQuoteSelector","exact":"The Curse of Dimensionality","prefix":".06437v1  [cs.LG]  11 Jun 2021• ","suffix":": In theory, according to the Cu"}]}]}
>```
>%%
>*%%PREFIX%%.06437v1  [cs.LG]  11 Jun 2021•%%HIGHLIGHT%% ==The Curse of Dimensionality== %%POSTFIX%%: In theory, according to the Cu*
>%%LINK%%[[#^0qjg6h4dobhl|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^0qjg6h4dobhl


>%%
>```annotation-json
>{"created":"2023-12-13T13:25:53.479Z","updated":"2023-12-13T13:25:53.479Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":3127,"end":3135},{"type":"TextQuoteSelector","exact":"Wrappers","prefix":" we follow the same structure:• ","suffix":" are feature selection methods w"}]}]}
>```
>%%
>*%%PREFIX%%we follow the same structure:•%%HIGHLIGHT%% ==Wrappers== %%POSTFIX%%are feature selection methods w*
>%%LINK%%[[#^rxui84w8on9|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^rxui84w8on9


>%%
>```annotation-json
>{"created":"2023-12-13T13:26:33.718Z","updated":"2023-12-13T13:26:33.718Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":3233,"end":3483},{"type":"TextQuoteSelector","exact":"This wrapping allows classification performance to drive the feature selectionprocess. This has the advantage of tying the feature selection to classifier performance butthis comes with a significant computational cost as very many classifier variant","prefix":"the feature selec-tion process. ","suffix":"s will beevaluated during the se"}]}]}
>```
>%%
>*%%PREFIX%%the feature selec-tion process.%%HIGHLIGHT%% ==This wrapping allows classification performance to drive the feature selectionprocess. This has the advantage of tying the feature selection to classifier performance butthis comes with a significant computational cost as very many classifier variant== %%POSTFIX%%s will beevaluated during the se*
>%%LINK%%[[#^7wqr0hp0lrb|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^7wqr0hp0lrb


>%%
>```annotation-json
>{"created":"2023-12-13T13:26:51.235Z","updated":"2023-12-13T13:26:51.235Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":3551,"end":3716},{"type":"TextQuoteSelector","exact":" use criteria other than classifier performance to guide featureselection. Typically a filter provides a feature ranking and then a selection policy uses thisranking","prefix":"ion.• Filters cover methods that","suffix":" to select a feature subset.• Em"}]}]}
>```
>%%
>*%%PREFIX%%ion.• Filters cover methods that%%HIGHLIGHT%% ==use criteria other than classifier performance to guide featureselection. Typically a filter provides a feature ranking and then a selection policy uses thisranking== %%POSTFIX%%to select a feature subset.• Em*
>%%LINK%%[[#^azp5xxkleef|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^azp5xxkleef


>%%
>```annotation-json
>{"created":"2023-12-13T13:27:06.807Z","updated":"2023-12-13T13:27:06.807Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":3746,"end":3868},{"type":"TextQuoteSelector","exact":"Embedded methods refer to any method where the feature selection emerges as a by-productof the classifier training process","prefix":"ng to select a feature subset.• ","suffix":". For instance, training a decis"}]}]}
>```
>%%
>*%%PREFIX%%ng to select a feature subset.•%%HIGHLIGHT%% ==Embedded methods refer to any method where the feature selection emerges as a by-productof the classifier training process== %%POSTFIX%%. For instance, training a decis*
>%%LINK%%[[#^rr16gd5k7zh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^rr16gd5k7zh


>%%
>```annotation-json
>{"created":"2023-12-13T13:32:43.766Z","updated":"2023-12-13T13:32:43.766Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":5199,"end":5223},{"type":"TextQuoteSelector","exact":" Curse of Dimensionality","prefix":"scussing the implications of the","suffix":"for ML. The term was coined by R"}]}]}
>```
>%%
>*%%PREFIX%%scussing the implications of the%%HIGHLIGHT%% ==Curse of Dimensionality== %%POSTFIX%%for ML. The term was coined by R*
>%%LINK%%[[#^vj41pk0a78e|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^vj41pk0a78e


>%%
>```annotation-json
>{"created":"2023-12-13T13:32:59.353Z","updated":"2023-12-13T13:32:59.353Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":5294,"end":5360},{"type":"TextQuoteSelector","exact":"number of phenomenaassociated with data described by many features","prefix":"Bellman in 1961 and refers to a ","suffix":" [1]. For our purposes there are"}]}]}
>```
>%%
>*%%PREFIX%%Bellman in 1961 and refers to a%%HIGHLIGHT%% ==number of phenomenaassociated with data described by many features== %%POSTFIX%%[1]. For our purposes there are*
>%%LINK%%[[#^ixykkh4mrga|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ixykkh4mrga


>%%
>```annotation-json
>{"created":"2023-12-13T13:34:12.983Z","updated":"2023-12-13T13:34:12.983Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":5417,"end":5659},{"type":"TextQuoteSelector","exact":"The first is that the number of samples required to build a model may increase exponentially with thenumber of features (dimensions). The second is that the variation in distance between arbitrary pointsdecreases as more dimensions are added.","prefix":"ere are two theoretical issues.2","suffix":"The first of these issues is ill"}]}]}
>```
>%%
>*%%PREFIX%%ere are two theoretical issues.2%%HIGHLIGHT%% ==The first is that the number of samples required to build a model may increase exponentially with thenumber of features (dimensions). The second is that the variation in distance between arbitrary pointsdecreases as more dimensions are added.== %%POSTFIX%%The first of these issues is ill*
>%%LINK%%[[#^ltutwn9jr2s|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ltutwn9jr2s


>%%
>```annotation-json
>{"created":"2023-12-13T13:37:42.015Z","updated":"2023-12-13T13:37:42.015Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":8318,"end":8466},{"type":"TextQuoteSelector","exact":"If we wish to get an assessment of generalisation performance for an ML system that might bedeployed then we need to hold back some data for testing","prefix":"guins 333 4 3Harry Potter 22 5 -","suffix":" (option (b) in Figure 4). If we"}]}]}
>```
>%%
>*%%PREFIX%%guins 333 4 3Harry Potter 22 5 -%%HIGHLIGHT%% ==If we wish to get an assessment of generalisation performance for an ML system that might bedeployed then we need to hold back some data for testing== %%POSTFIX%%(option (b) in Figure 4). If we*
>%%LINK%%[[#^kfjeo26x9f|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^kfjeo26x9f


>%%
>```annotation-json
>{"created":"2023-12-13T13:37:56.579Z","updated":"2023-12-13T13:37:56.579Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":8492,"end":8559},{"type":"TextQuoteSelector","exact":" If we wish toassess a few different feature selection alternatives","prefix":"esting (option (b) in Figure 4).","suffix":" as part of the model developmen"}]}]}
>```
>%%
>*%%PREFIX%%esting (option (b) in Figure 4).%%HIGHLIGHT%% ==If we wish toassess a few different feature selection alternatives== %%POSTFIX%%as part of the model developmen*
>%%LINK%%[[#^oek8clxgjmc|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^oek8clxgjmc


>%%
>```annotation-json
>{"created":"2023-12-13T13:38:08.771Z","updated":"2023-12-13T13:38:08.771Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":8660,"end":8703},{"type":"TextQuoteSelector","exact":" cross validation is the most effective way","prefix":"he confines of training data and","suffix":"to do this (option (c)). Most of"}]}]}
>```
>%%
>*%%PREFIX%%he confines of training data and%%HIGHLIGHT%% ==cross validation is the most effective way== %%POSTFIX%%to do this (option (c)). Most of*
>%%LINK%%[[#^w3iz8hwzmk8|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^w3iz8hwzmk8


>%%
>```annotation-json
>{"created":"2023-12-13T13:38:41.397Z","updated":"2023-12-13T13:38:41.397Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":8991,"end":9079},{"type":"TextQuoteSelector","exact":" deployment of anML system then all the available data can be used for feature selection","prefix":"feature selection as part of the","suffix":" (option (a) in Figure 4).Figure"}]}]}
>```
>%%
>*%%PREFIX%%feature selection as part of the%%HIGHLIGHT%% ==deployment of anML system then all the available data can be used for feature selection== %%POSTFIX%%(option (a) in Figure 4).Figure*
>%%LINK%%[[#^sespcrw5gw|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^sespcrw5gw


>%%
>```annotation-json
>{"created":"2023-12-13T13:40:27.844Z","updated":"2023-12-13T13:40:27.844Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":9436,"end":9735},{"type":"TextQuoteSelector","exact":"Unless otherwise stated, the classifier used in the evaluations in k-Nearest Neighbour (k-NN) [5].k-NN is used because it is probably the classifier in popular use most susceptible to noisy or redundantfeatures. So the impact of feature selection will be most evident when it is used in evaluations.","prefix":"ning data for Feature Selection.","suffix":"Before proceeding we need to int"}]}]}
>```
>%%
>*%%PREFIX%%ning data for Feature Selection.%%HIGHLIGHT%% ==Unless otherwise stated, the classifier used in the evaluations in k-Nearest Neighbour (k-NN) [5].k-NN is used because it is probably the classifier in popular use most susceptible to noisy or redundantfeatures. So the impact of feature selection will be most evident when it is used in evaluations.== %%POSTFIX%%Before proceeding we need to int*
>%%LINK%%[[#^sa13v5c5owq|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^sa13v5c5owq


>%%
>```annotation-json
>{"created":"2023-12-13T13:44:20.722Z","updated":"2023-12-13T13:44:20.722Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":9735,"end":10280},{"type":"TextQuoteSelector","exact":"Before proceeding we need to introduce the notation that will be used throughout the paper. Assumewe have a dataset D made up of n data samples. D = 〈X,y〉 where y are the class labels. Theexamples are described by a set of features F where p = |F|so there are n objects described byp features. So the dimensions are Xn×p and yp. The objective is to identify a subset S ⊂ F thatcaptures the important information in the dataset. In supervised ML the classifiers would work withdata represented by a reduced set of features 〈X′n×k,y〉 where k = |S|","prefix":" when it is used in evaluations.","suffix":".1 https://archive.ics.uci.edu/2"}]}]}
>```
>%%
>*%%PREFIX%%when it is used in evaluations.%%HIGHLIGHT%% ==Before proceeding we need to introduce the notation that will be used throughout the paper. Assumewe have a dataset D made up of n data samples. D = 〈X,y〉 where y are the class labels. Theexamples are described by a set of features F where p = |F|so there are n objects described byp features. So the dimensions are Xn×p and yp. The objective is to identify a subset S ⊂ F thatcaptures the important information in the dataset. In supervised ML the classifiers would work withdata represented by a reduced set of features 〈X′n×k,y〉 where k = |S|== %%POSTFIX%%.1 https://archive.ics.uci.edu/2*
>%%LINK%%[[#^d8rgkje14fn|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^d8rgkje14fn


>%%
>```annotation-json
>{"created":"2023-12-13T13:45:27.802Z","updated":"2023-12-13T13:45:27.802Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":10417,"end":10574},{"type":"TextQuoteSelector","exact":"Wrappers versus Filters: (a) With Wrappers the classifier is wrapped in the search process.(b) A Filter strategy uses a separate evaluation to score features","prefix":"lections/harry-potter4Figure 5: ","suffix":"4 Feature Selection using Wrappe"}]}]}
>```
>%%
>*%%PREFIX%%lections/harry-potter4Figure 5:%%HIGHLIGHT%% ==Wrappers versus Filters: (a) With Wrappers the classifier is wrapped in the search process.(b) A Filter strategy uses a separate evaluation to score features== %%POSTFIX%%4 Feature Selection using Wrappe*
>%%LINK%%[[#^nvcll2bfb6|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^nvcll2bfb6


>%%
>```annotation-json
>{"created":"2023-12-13T13:46:34.100Z","updated":"2023-12-13T13:46:34.100Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":10608,"end":10810},{"type":"TextQuoteSelector","exact":"So the objective with feature selection is to identify a feature subset S ⊂ F to represent the data.If |F|is small we could in theory try out all possible subsets of features and select the best subset.","prefix":"Feature Selection using Wrappers","suffix":"In this case ‘try out’ would mea"}]}]}
>```
>%%
>*%%PREFIX%%Feature Selection using Wrappers%%HIGHLIGHT%% ==So the objective with feature selection is to identify a feature subset S ⊂ F to represent the data.If |F|is small we could in theory try out all possible subsets of features and select the best subset.== %%POSTFIX%%In this case ‘try out’ would mea*
>%%LINK%%[[#^7gp140ig9so|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^7gp140ig9so


>%%
>```annotation-json
>{"created":"2023-12-13T13:49:12.741Z","updated":"2023-12-13T13:49:12.741Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":11177,"end":11343},{"type":"TextQuoteSelector","exact":"Nevertheless this is how a Wrapper feature selection strategy works with the important modificationthat the search can be greedy or stochastic rather than exhaustive.","prefix":"arch quickly becomes impossible.","suffix":" The general idea is shown in Fi"}]}]}
>```
>%%
>*%%PREFIX%%arch quickly becomes impossible.%%HIGHLIGHT%% ==Nevertheless this is how a Wrapper feature selection strategy works with the important modificationthat the search can be greedy or stochastic rather than exhaustive.== %%POSTFIX%%The general idea is shown in Fi*
>%%LINK%%[[#^io54pipfruf|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^io54pipfruf


>%%
>```annotation-json
>{"created":"2023-12-13T13:57:09.097Z","updated":"2023-12-13T13:57:09.097Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":11703,"end":11760},{"type":"TextQuoteSelector","exact":"Exhaustive Search evaluates every possible feature subset","prefix":"ategies used withWrappers are:• ","suffix":". If the number of features tobe"}]}]}
>```
>%%
>*%%PREFIX%%ategies used withWrappers are:•%%HIGHLIGHT%% ==Exhaustive Search evaluates every possible feature subset== %%POSTFIX%%. If the number of features tobe*
>%%LINK%%[[#^2ydg7uoq5y2|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^2ydg7uoq5y2


>%%
>```annotation-json
>{"created":"2023-12-13T13:58:31.294Z","updated":"2023-12-13T13:58:31.294Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":11993,"end":12125},{"type":"TextQuoteSelector","exact":"Sequential Forward Selection (SFS) starts with no features selected and all classifiersincorporating a single feature are considered","prefix":"e search willnot be practical.• ","suffix":" (see Figure 6 (a)). The best of"}]}]}
>```
>%%
>*%%PREFIX%%e search willnot be practical.•%%HIGHLIGHT%% ==Sequential Forward Selection (SFS) starts with no features selected and all classifiersincorporating a single feature are considered== %%POSTFIX%%(see Figure 6 (a)). The best of*
>%%LINK%%[[#^132ru5ij49s|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^132ru5ij49s


>%%
>```annotation-json
>{"created":"2023-12-13T13:59:00.900Z","updated":"2023-12-13T13:59:00.900Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":12146,"end":12246},{"type":"TextQuoteSelector","exact":"The best of these is selectedand then two feature combinations including this feature are evaluated.","prefix":" considered (see Figure 6 (a)). ","suffix":" This processproceeds, adding th"}]}]}
>```
>%%
>*%%PREFIX%%considered (see Figure 6 (a)).%%HIGHLIGHT%% ==The best of these is selectedand then two feature combinations including this feature are evaluated.== %%POSTFIX%%This processproceeds, adding th*
>%%LINK%%[[#^cfzrwj2ia0e|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^cfzrwj2ia0e


>%%
>```annotation-json
>{"created":"2023-12-13T13:59:29.722Z","updated":"2023-12-13T13:59:29.722Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":12353,"end":12378},{"type":"TextQuoteSelector","exact":"Backward Elimination (BE)","prefix":"rther improvements can bemade.• ","suffix":" proceeds in the opposite direct"}]}]}
>```
>%%
>*%%PREFIX%%rther improvements can bemade.•%%HIGHLIGHT%% ==Backward Elimination (BE)== %%POSTFIX%%proceeds in the opposite direct*
>%%LINK%%[[#^ti502i7x2j|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ti502i7x2j


>%%
>```annotation-json
>{"created":"2023-12-13T13:59:50.962Z","updated":"2023-12-13T13:59:50.962Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":12421,"end":12567},{"type":"TextQuoteSelector","exact":" it starts with allfeatures selected, considers the options with one feature deleted, selects the best of theseand continues to eliminate features","prefix":"n the opposite direction to FSS,","suffix":". Again, the process is terminat"}]}]}
>```
>%%
>*%%PREFIX%%n the opposite direction to FSS,%%HIGHLIGHT%% ==it starts with allfeatures selected, considers the options with one feature deleted, selects the best of theseand continues to eliminate features== %%POSTFIX%%. Again, the process is terminat*
>%%LINK%%[[#^szcjqxolp8|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^szcjqxolp8


>%%
>```annotation-json
>{"created":"2023-12-13T14:00:29.598Z","updated":"2023-12-13T14:00:29.598Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":12636,"end":12856},{"type":"TextQuoteSelector","exact":"Stochastic Search methods such as genetic algorithms or simulated annealing can readilybe applied to Wrapper feature selection. Each state can be defined by a feature mask onwhich crossover and mutation can be performed ","prefix":"en no improvementscan be made.• ","suffix":"[22]. Given this convenient repr"}]}]}
>```
>%%
>*%%PREFIX%%en no improvementscan be made.•%%HIGHLIGHT%% ==Stochastic Search methods such as genetic algorithms or simulated annealing can readilybe applied to Wrapper feature selection. Each state can be defined by a feature mask onwhich crossover and mutation can be performed== %%POSTFIX%%[22]. Given this convenient repr*
>%%LINK%%[[#^sji9awnbj9h|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^sji9awnbj9h


>%%
>```annotation-json
>{"created":"2023-12-13T14:02:18.221Z","updated":"2023-12-13T14:02:18.221Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":13251,"end":13545},{"type":"TextQuoteSelector","exact":"SFS starts with an empty set andproceeds forward considering classifiers built on single features. The best of these is selected andthen pairs of features incorporating this feature are considered. The process could terminate whenthe addition of a new feature doesn’t result in any improvement.","prefix":"e subsets as shown in Figure 6. ","suffix":" As the name suggests, BackwardE"}]}]}
>```
>%%
>*%%PREFIX%%e subsets as shown in Figure 6.%%HIGHLIGHT%% ==SFS starts with an empty set andproceeds forward considering classifiers built on single features. The best of these is selected andthen pairs of features incorporating this feature are considered. The process could terminate whenthe addition of a new feature doesn’t result in any improvement.== %%POSTFIX%%As the name suggests, BackwardE*
>%%LINK%%[[#^b4m8gbk0jao|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^b4m8gbk0jao


>%%
>```annotation-json
>{"created":"2023-12-13T14:02:57.799Z","updated":"2023-12-13T14:02:57.799Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":13568,"end":13587},{"type":"TextQuoteSelector","exact":"BackwardElimination","prefix":"rovement. As the name suggests, ","suffix":" works in the opposite direction"}]}]}
>```
>%%
>*%%PREFIX%%rovement. As the name suggests,%%HIGHLIGHT%% ==BackwardElimination== %%POSTFIX%%works in the opposite direction*
>%%LINK%%[[#^o4gjvzjy33|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^o4gjvzjy33


>%%
>```annotation-json
>{"created":"2023-12-13T14:03:05.989Z","updated":"2023-12-13T14:03:05.989Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":13621,"end":13825},{"type":"TextQuoteSelector","exact":"It starts with a full set of features (Figure 6 (b)) andeliminates the least useful feature at each step. For both SFS and BE, the feature subsets are evaluatedusing cross-validation on the training data.","prefix":"orks in the opposite direction. ","suffix":" As stated in Section 3, the cla"}]}]}
>```
>%%
>*%%PREFIX%%orks in the opposite direction.%%HIGHLIGHT%% ==It starts with a full set of features (Figure 6 (b)) andeliminates the least useful feature at each step. For both SFS and BE, the feature subsets are evaluatedusing cross-validation on the training data.== %%POSTFIX%%As stated in Section 3, the cla*
>%%LINK%%[[#^c7qipqm1sm9|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^c7qipqm1sm9


>%%
>```annotation-json
>{"created":"2023-12-13T14:05:32.592Z","updated":"2023-12-13T14:05:32.592Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":13934,"end":14021},{"type":"TextQuoteSelector","exact":". SFS is inclined to require less computa-tion as the models being evaluated are smalle","prefix":"own advantages and disadvantages","suffix":"r, typically a classifier with a"}]}]}
>```
>%%
>*%%PREFIX%%own advantages and disadvantages%%HIGHLIGHT%% ==. SFS is inclined to require less computa-tion as the models being evaluated are smalle== %%POSTFIX%%r, typically a classifier with a*
>%%LINK%%[[#^d1n0t7wamcs|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^d1n0t7wamcs


>%%
>```annotation-json
>{"created":"2023-12-13T14:06:01.851Z","updated":"2023-12-13T14:06:01.851Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":14511,"end":14724},{"type":"TextQuoteSelector","exact":"SFS is inclined to select less features; this parsinomy is typicallyan advantage. On the other hand, because BE starts with larger feature sets, it can do a better job ofassessing how features work in combination.","prefix":"ke less time to train and test. ","suffix":"In the Appendix a link is provid"}]}]}
>```
>%%
>*%%PREFIX%%ke less time to train and test.%%HIGHLIGHT%% ==SFS is inclined to select less features; this parsinomy is typicallyan advantage. On the other hand, because BE starts with larger feature sets, it can do a better job ofassessing how features work in combination.== %%POSTFIX%%In the Appendix a link is provid*
>%%LINK%%[[#^gutybl5r2or|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^gutybl5r2or


>%%
>```annotation-json
>{"created":"2023-12-13T14:18:30.385Z","updated":"2023-12-13T14:18:30.385Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":15142,"end":15330},{"type":"TextQuoteSelector","exact":"FS selects seven features and 11 areselected by BE. Both feature subsets result in improved accuracy on the training data but only theSFS subset results in better accuracy on the test data","prefix":" BE are shown in Figure 7 (b). S","suffix":". Indeed the gap between train a"}]}]}
>```
>%%
>*%%PREFIX%%BE are shown in Figure 7 (b). S%%HIGHLIGHT%% ==FS selects seven features and 11 areselected by BE. Both feature subsets result in improved accuracy on the training data but only theSFS subset results in better accuracy on the test data== %%POSTFIX%%. Indeed the gap between train a*
>%%LINK%%[[#^714o0y86i9m|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^714o0y86i9m


>%%
>```annotation-json
>{"created":"2023-12-13T14:18:53.318Z","updated":"2023-12-13T14:18:53.318Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":15542,"end":15626},{"type":"TextQuoteSelector","exact":"Indeed overfitting is recognised to be aproblem with Wrapper-based feature selection","prefix":"ost of generalisation accuracy. ","suffix":" [22].5 Filter StrategiesFigure "}]}]}
>```
>%%
>*%%PREFIX%%ost of generalisation accuracy.%%HIGHLIGHT%% ==Indeed overfitting is recognised to be aproblem with Wrapper-based feature selection== %%POSTFIX%%[22].5 Filter StrategiesFigure*
>%%LINK%%[[#^7viwxoe01uh|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^7viwxoe01uh


>%%
>```annotation-json
>{"created":"2023-12-13T14:20:01.974Z","updated":"2023-12-13T14:20:01.974Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":15783,"end":15895},{"type":"TextQuoteSelector","exact":"Filter strategies do not use the classifier for feature selection, insteada separate evaluation function is used","prefix":"rocess. Figure 5 (b) shows that ","suffix":". The fact that Filters are inde"}]}]}
>```
>%%
>*%%PREFIX%%rocess. Figure 5 (b) shows that%%HIGHLIGHT%% ==Filter strategies do not use the classifier for feature selection, insteada separate evaluation function is used== %%POSTFIX%%. The fact that Filters are inde*
>%%LINK%%[[#^wac5lxgkqv|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^wac5lxgkqv


>%%
>```annotation-json
>{"created":"2023-12-13T14:20:15.629Z","updated":"2023-12-13T14:20:15.629Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":15972,"end":16114},{"type":"TextQuoteSelector","exact":" It means that Filters can be much faster than Wrappers but the selected features may not bein tune with the inductive bias of the classifier.","prefix":"e classifier is a mixedblessing.","suffix":"In the next subsection we provid"}]}]}
>```
>%%
>*%%PREFIX%%e classifier is a mixedblessing.%%HIGHLIGHT%% ==It means that Filters can be much faster than Wrappers but the selected features may not bein tune with the inductive bias of the classifier.== %%POSTFIX%%In the next subsection we provid*
>%%LINK%%[[#^x9y7bdomd4h|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^x9y7bdomd4h


>%%
>```annotation-json
>{"created":"2023-12-13T14:20:59.271Z","updated":"2023-12-13T14:20:59.271Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":16373,"end":16480},{"type":"TextQuoteSelector","exact":"A basic Filter will entail a feature scoring mechanism and then a selection strategy based on thesescores. ","prefix":" recent years.65.1 Basic Filters","suffix":"The scoring mechanism needs to q"}]}]}
>```
>%%
>*%%PREFIX%%recent years.65.1 Basic Filters%%HIGHLIGHT%% ==A basic Filter will entail a feature scoring mechanism and then a selection strategy based on thesescores.== %%POSTFIX%%The scoring mechanism needs to q*
>%%LINK%%[[#^5nugzsm5od|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^5nugzsm5od


>%%
>```annotation-json
>{"created":"2023-12-13T14:21:47.295Z","updated":"2023-12-13T14:21:47.295Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":16575,"end":16755},{"type":"TextQuoteSelector","exact":"The selection strategy might be:• Select the top ranked k features,• Select top 50%,• Select features with scores > 50% of the maximum score,• Select features with non-zero scores.","prefix":"e feature has about theoutcome. ","suffix":"In this analysis we consider the"}]}]}
>```
>%%
>*%%PREFIX%%e feature has about theoutcome.%%HIGHLIGHT%% ==The selection strategy might be:• Select the top ranked k features,• Select top 50%,• Select features with scores > 50% of the maximum score,• Select features with non-zero scores.== %%POSTFIX%%In this analysis we consider the*
>%%LINK%%[[#^xsvtsmmp6pb|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^xsvtsmmp6pb


>%%
>```annotation-json
>{"created":"2023-12-13T14:22:52.237Z","updated":"2023-12-13T14:22:52.237Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":16755,"end":16841},{"type":"TextQuoteSelector","exact":"In this analysis we consider the Chi-square statistic and information gain for scoring","prefix":"t features with non-zero scores.","suffix":". The Chi-squarestatistic is a m"}]}]}
>```
>%%
>*%%PREFIX%%t features with non-zero scores.%%HIGHLIGHT%% ==In this analysis we consider the Chi-square statistic and information gain for scoring== %%POSTFIX%%. The Chi-squarestatistic is a m*
>%%LINK%%[[#^a3ujdechtf8|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^a3ujdechtf8


>%%
>```annotation-json
>{"created":"2023-12-13T14:23:05.135Z","updated":"2023-12-13T14:23:05.135Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":16842,"end":16932},{"type":"TextQuoteSelector","exact":" The Chi-squarestatistic is a measure of independence between a feature and the class labe","prefix":"nd information gain for scoring.","suffix":"l. If samples are organisedinto "}]}]}
>```
>%%
>*%%PREFIX%%nd information gain for scoring.%%HIGHLIGHT%% ==The Chi-squarestatistic is a measure of independence between a feature and the class labe== %%POSTFIX%%l. If samples are organisedinto*
>%%LINK%%[[#^6ef84wd7z83|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^6ef84wd7z83


>%%
>```annotation-json
>{"created":"2023-12-13T14:27:03.309Z","updated":"2023-12-13T14:27:03.309Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":17253,"end":17325},{"type":"TextQuoteSelector","exact":"The Chi-square statistic allows us to quantify this:χ2 =m∑i=1(Oi −Ei)2Ei","prefix":"der is predictive of handedness.","suffix":"(1)The statistic is a sum over t"}]}]}
>```
>%%
>*%%PREFIX%%der is predictive of handedness.%%HIGHLIGHT%% ==The Chi-square statistic allows us to quantify this:χ2 =m∑i=1(Oi −Ei)2Ei== %%POSTFIX%%(1)The statistic is a sum over t*
>%%LINK%%[[#^1kwl9800kdr|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^1kwl9800kdr


>%%
>```annotation-json
>{"created":"2023-12-13T19:31:18.606Z","updated":"2023-12-13T19:31:18.606Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":18091,"end":18215},{"type":"TextQuoteSelector","exact":"nformation gain is an alternative information-theoretic measure quantifying the information a featurecontains about a class ","prefix":", in (b) there is a dependence.I","suffix":"[16]. In Figure 8 (b) by knowing"}]}]}
>```
>%%
>*%%PREFIX%%, in (b) there is a dependence.I%%HIGHLIGHT%% ==nformation gain is an alternative information-theoretic measure quantifying the information a featurecontains about a class== %%POSTFIX%%[16]. In Figure 8 (b) by knowing*
>%%LINK%%[[#^mw5vzmka35i|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^mw5vzmka35i


>%%
>```annotation-json
>{"created":"2023-12-13T19:43:09.444Z","updated":"2023-12-13T19:43:09.444Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":18648,"end":18771},{"type":"TextQuoteSelector","exact":"As with the Chi-square statistic, information gain (I-Gain) allows us to rank features for the purposeof feature selection.","prefix":"D) − ∑v∈values(f)|Sv|S H(Dv) (3)","suffix":" This is illustrated in Figure 9"}]}]}
>```
>%%
>*%%PREFIX%%D) − ∑v∈values(f)|Sv|S H(Dv) (3)%%HIGHLIGHT%% ==As with the Chi-square statistic, information gain (I-Gain) allows us to rank features for the purposeof feature selection.== %%POSTFIX%%This is illustrated in Figure 9*
>%%LINK%%[[#^5gtipaf84a6|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^5gtipaf84a6


>%%
>```annotation-json
>{"created":"2023-12-13T19:54:06.180Z","updated":"2023-12-13T19:54:06.180Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":20138,"end":20294},{"type":"TextQuoteSelector","exact":"Thissuggests a hybrid Filter/Wrapper strategy whereby a Filter is used to rank the features and then aWrapper is used to identify the optimum feature subset","prefix":"eatures may damage performance. ","suffix":".This hybrid strategy is shown i"}]}]}
>```
>%%
>*%%PREFIX%%eatures may damage performance.%%HIGHLIGHT%% ==Thissuggests a hybrid Filter/Wrapper strategy whereby a Filter is used to rank the features and then aWrapper is used to identify the optimum feature subset== %%POSTFIX%%.This hybrid strategy is shown i*
>%%LINK%%[[#^ncp4p5w1lw|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ncp4p5w1lw


>%%
>```annotation-json
>{"created":"2023-12-13T19:55:57.287Z","updated":"2023-12-13T19:55:57.287Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":20932,"end":21094},{"type":"TextQuoteSelector","exact":"The Relief family of algorithms deserve mention because, as Filter methods, they have the advantageof speed while scoring features in the context of other feature","prefix":".2 Relief Algorithm and Variants","suffix":"s [17, 18]. Relief algorithms be"}]}]}
>```
>%%
>*%%PREFIX%%.2 Relief Algorithm and Variants%%HIGHLIGHT%% ==The Relief family of algorithms deserve mention because, as Filter methods, they have the advantageof speed while scoring features in the context of other feature== %%POSTFIX%%s [17, 18]. Relief algorithms be*
>%%LINK%%[[#^7cy2tk6k8ye|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^7cy2tk6k8ye


>%%
>```annotation-json
>{"created":"2023-12-13T19:56:55.504Z","updated":"2023-12-13T19:56:55.504Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":21429,"end":21648},{"type":"TextQuoteSelector","exact":"For the nearMiss (nM) increment the feature weights; weights for unmatching featureswill be incremented more.• For the nearHit (nH) decrement the feature weights; weights for unmatching features willbe decremented more.","prefix":"Then the general principle is:• ","suffix":"The idea is that this will pull "}]}]}
>```
>%%
>*%%PREFIX%%Then the general principle is:•%%HIGHLIGHT%% ==For the nearMiss (nM) increment the feature weights; weights for unmatching featureswill be incremented more.• For the nearHit (nH) decrement the feature weights; weights for unmatching features willbe decremented more.== %%POSTFIX%%The idea is that this will pull*
>%%LINK%%[[#^i57o19dyov|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^i57o19dyov


>%%
>```annotation-json
>{"created":"2023-12-13T19:57:36.423Z","updated":"2023-12-13T19:57:36.423Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":21212,"end":21426},{"type":"TextQuoteSelector","exact":"he idea is to takeeach sample in the dataset (or a subset) and find its nearest neighbour of the same class and nearestunlike neighbour - these are termed the nearHit and the nearMiss. Then the general principle is","prefix":"d to score or weight features. T","suffix":":• For the nearMiss (nM) increme"}]}]}
>```
>%%
>*%%PREFIX%%d to score or weight features. T%%HIGHLIGHT%% ==he idea is to takeeach sample in the dataset (or a subset) and find its nearest neighbour of the same class and nearestunlike neighbour - these are termed the nearHit and the nearMiss. Then the general principle is== %%POSTFIX%%:• For the nearMiss (nM) increme*
>%%LINK%%[[#^xos6128khgf|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^xos6128khgf


>%%
>```annotation-json
>{"created":"2023-12-13T19:58:09.062Z","updated":"2023-12-13T19:58:09.062Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":21648,"end":21751},{"type":"TextQuoteSelector","exact":"The idea is that this will pull matching instances closer together and push unmatching instances apart.","prefix":"eatures willbe decremented more.","suffix":"This is illustrated in the 2D ex"}]}]}
>```
>%%
>*%%PREFIX%%eatures willbe decremented more.%%HIGHLIGHT%% ==The idea is that this will pull matching instances closer together and push unmatching instances apart.== %%POSTFIX%%This is illustrated in the 2D ex*
>%%LINK%%[[#^e77tgvm1fhq|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^e77tgvm1fhq


>%%
>```annotation-json
>{"created":"2023-12-13T19:58:39.618Z","updated":"2023-12-13T19:58:39.618Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":22376,"end":22537},{"type":"TextQuoteSelector","exact":"with the following update function where x is the query, nM is the nearMiss, nH is the nearHitand wf is the weight for feature f.wf ←wf −(xf −nHf)2 + (xf −nMf)2 ","prefix":"ture subset is marked with an X.","suffix":"(4)This achieves what we want be"}]}]}
>```
>%%
>*%%PREFIX%%ture subset is marked with an X.%%HIGHLIGHT%% ==with the following update function where x is the query, nM is the nearMiss, nH is the nearHitand wf is the weight for feature f.wf ←wf −(xf −nHf)2 + (xf −nMf)2== %%POSTFIX%%(4)This achieves what we want be*
>%%LINK%%[[#^wv4nagirokb|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^wv4nagirokb


>%%
>```annotation-json
>{"created":"2023-12-13T20:12:00.192Z","updated":"2023-12-13T20:12:00.192Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":22847,"end":22963},{"type":"TextQuoteSelector","exact":"This process is repeated multiple times to produce a set of feature weights with ‘good’ features havinghigh weights.","prefix":" f1 should be preferred over f2.","suffix":" Implementation details of the R"}]}]}
>```
>%%
>*%%PREFIX%%f1 should be preferred over f2.%%HIGHLIGHT%% ==This process is repeated multiple times to produce a set of feature weights with ‘good’ features havinghigh weights.== %%POSTFIX%%Implementation details of the R*
>%%LINK%%[[#^jj8tcx8m6k|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^jj8tcx8m6k


>%%
>```annotation-json
>{"created":"2023-12-13T20:12:42.538Z","updated":"2023-12-13T20:12:42.538Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":23889,"end":24139},{"type":"TextQuoteSelector","exact":"Correlation Based feature selection (CFS) is a filter strategy that relies on the principle that \"Agood feature subset is one that contains features highly correlated with (predictive of) the class,yet uncorrelated with (not predictive of) each other","prefix":"relation-Based Feature Selection","suffix":"\" [12]. The feature-class correl"}]}]}
>```
>%%
>*%%PREFIX%%relation-Based Feature Selection%%HIGHLIGHT%% ==Correlation Based feature selection (CFS) is a filter strategy that relies on the principle that "Agood feature subset is one that contains features highly correlated with (predictive of) the class,yet uncorrelated with (not predictive of) each other== %%POSTFIX%%" [12]. The feature-class correl*
>%%LINK%%[[#^77um3cssvdj|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^77um3cssvdj


>%%
>```annotation-json
>{"created":"2023-12-13T20:14:09.294Z","updated":"2023-12-13T20:14:09.294Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":24147,"end":24491},{"type":"TextQuoteSelector","exact":"The feature-class correlation indicateshow representative of the class that feature is while the feature-feature correlation indicates anyredundancies between the features. CFS works by assigning a merit value based on feature-class andfeature-feature correlations to each feature subset which becomes the measure by which subsets areevaluated.","prefix":"redictive of) each other\" [12]. ","suffix":" The merit score for a feature s"}]}]}
>```
>%%
>*%%PREFIX%%redictive of) each other" [12].%%HIGHLIGHT%% ==The feature-class correlation indicateshow representative of the class that feature is while the feature-feature correlation indicates anyredundancies between the features. CFS works by assigning a merit value based on feature-class andfeature-feature correlations to each feature subset which becomes the measure by which subsets areevaluated.== %%POSTFIX%%The merit score for a feature s*
>%%LINK%%[[#^kr75mugr79|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^kr75mugr79


>%%
>```annotation-json
>{"created":"2023-12-13T20:19:33.477Z","updated":"2023-12-13T20:19:33.477Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":42412,"end":42827},{"type":"TextQuoteSelector","exact":"Preliminary Analysis: Use RF feature importance to rank all features (e.g. as shown inFigure 17). This will provide some insight into what features are important and it may alsoindicate features that can be dropped from further consideration. We recommend the RFmethod as it considers features in context. If the objective of the exercise is to gain an insightinto the data then the analysis may stop at this point.","prefix":"e selection on a new dataset.1. ","suffix":"2. Subset Selection: If the obje"}]}]}
>```
>%%
>*%%PREFIX%%e selection on a new dataset.1.%%HIGHLIGHT%% ==Preliminary Analysis: Use RF feature importance to rank all features (e.g. as shown inFigure 17). This will provide some insight into what features are important and it may alsoindicate features that can be dropped from further consideration. We recommend the RFmethod as it considers features in context. If the objective of the exercise is to gain an insightinto the data then the analysis may stop at this point.== %%POSTFIX%%2. Subset Selection: If the obje*
>%%LINK%%[[#^ol5i9igp77|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^ol5i9igp77


>%%
>```annotation-json
>{"created":"2023-12-13T20:19:50.084Z","updated":"2023-12-13T20:19:50.084Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":42830,"end":43015},{"type":"TextQuoteSelector","exact":"Subset Selection: If the objective is to identify an effective subset for classification then asubset selection strategy is required. The main argument against a Wrapper strategy is the","prefix":"lysis may stop at this point.2. ","suffix":"17computational cost. With advan"}]}]}
>```
>%%
>*%%PREFIX%%lysis may stop at this point.2.%%HIGHLIGHT%% ==Subset Selection: If the objective is to identify an effective subset for classification then asubset selection strategy is required. The main argument against a Wrapper strategy is the== %%POSTFIX%%17computational cost. With advan*
>%%LINK%%[[#^5gdfv6d4yy|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^5gdfv6d4yy


>%%
>```annotation-json
>{"created":"2023-12-13T20:19:58.272Z","updated":"2023-12-13T20:19:58.272Z","document":{"title":"2106.06437.pdf","link":[{"href":"urn:x-pdf:34083b2a9ef4286148b97fd42201dcbc"},{"href":"vault:/Slides/IML/Readings/2106.06437.pdf"}],"documentFingerprint":"34083b2a9ef4286148b97fd42201dcbc"},"uri":"vault:/Slides/IML/Readings/2106.06437.pdf","target":[{"source":"vault:/Slides/IML/Readings/2106.06437.pdf","selector":[{"type":"TextPositionSelector","start":43017,"end":43402},{"type":"TextQuoteSelector","exact":"computational cost. With advances in computing resources this is now less of an issue sowe recommend a Wrapper strategy as described in Section 4. If the number of features stillin consideration is not high then BE should be considered. If the set of possible features islarge then SFS may be the pragmatic choice. The hybrid strategy described in Section 5.1.1could also be considered","prefix":"inst a Wrapper strategy is the17","suffix":".So our overall recommendation f"}]}]}
>```
>%%
>*%%PREFIX%%inst a Wrapper strategy is the17%%HIGHLIGHT%% ==computational cost. With advances in computing resources this is now less of an issue sowe recommend a Wrapper strategy as described in Section 4. If the number of features stillin consideration is not high then BE should be considered. If the set of possible features islarge then SFS may be the pragmatic choice. The hybrid strategy described in Section 5.1.1could also be considered== %%POSTFIX%%.So our overall recommendation f*
>%%LINK%%[[#^rbnop5ptrda|show annotation]]
>%%COMMENT%%
>
>%%TAGS%%
>
^rbnop5ptrda
